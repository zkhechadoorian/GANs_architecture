{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUoE-iDYGJUj"
      },
      "source": [
        "# üéâ Party Time!  \n",
        "This notebook provides a comprehensive overview of autoencoders and their applications in deep learning. You will learn how to build and train autoencoders for dimensionality reduction, image denoising, and anomaly detection. The workflow includes:\n",
        "\n",
        "- [**Basic Autoencoder**](#Autoencoders) üß†: Learn the fundamentals by compressing and reconstructing Fashion MNIST images using dense layers.\n",
        "- [**Image Denoising**](#Second-example-Image-denoising) üßπüñºÔ∏è: Apply convolutional autoencoders to remove noise from images, demonstrating practical data cleaning techniques.\n",
        "- [**Anomaly Detection**](#Third-example-Anomaly-detection) üö®üìà: Use autoencoders to identify abnormal patterns in ECG time series data, illustrating unsupervised anomaly detection.\n",
        "- [**Image-to-Image Translation (pix2pix)**](#pix2pix-Image-to-image-translation-with-a-conditional-GAN) üè¢‚û°Ô∏èüèôÔ∏è: Explore conditional GANs for translating architectural label images into realistic building facades.\n",
        "\n",
        "Each section includes code, explanations, and visualizations to help you understand the concepts and implementation details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfNT-mlFwxVM"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZuApL56Mny"
      },
      "source": [
        "We will explore autoencoders with three examples: the basics, image denoising, and anomaly detection.\n",
        "\n",
        "An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "OQ7f-kIXGPuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:23.516087Z",
          "iopub.status.busy": "2024-08-16T05:08:23.515550Z",
          "iopub.status.idle": "2024-08-16T05:08:26.461467Z",
          "shell.execute_reply": "2024-08-16T05:08:26.460737Z"
        },
        "id": "YfIk2es3hJEd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load the dataset\n",
        "To start, you will train the basic autoencoder using the Fashion MNIST dataset. Each image in this dataset is 28x28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:26.465909Z",
          "iopub.status.busy": "2024-08-16T05:08:26.465480Z",
          "iopub.status.idle": "2024-08-16T05:08:26.951978Z",
          "shell.execute_reply": "2024-08-16T05:08:26.951144Z"
        },
        "id": "YZm503-I_tji"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEdCXSwCoKok"
      },
      "source": [
        "### First example: Basic autoencoder\n",
        "![Basic autoencoder results](https://www.tensorflow.org/static/tutorials/generative/images/intro_autoencoder_result.png)\n",
        "\n",
        "Define an autoencoder with two Dense layers: an `encoder`, which compresses the images into a 64 dimensional latent vector, and a `decoder`, that reconstructs the original image from the latent space.\n",
        "\n",
        "To define your model, use the [Keras Model Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:26.955535Z",
          "iopub.status.busy": "2024-08-16T05:08:26.955262Z",
          "iopub.status.idle": "2024-08-16T05:08:29.196992Z",
          "shell.execute_reply": "2024-08-16T05:08:29.196168Z"
        },
        "id": "0MUxidpyChjX"
      },
      "outputs": [],
      "source": [
        "# Define an Autoencoder class inheriting from tf.keras.Model\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim, shape):\n",
        "    super(Autoencoder, self).__init__()  # Initialize the base class\n",
        "    self.latent_dim = latent_dim         # Store the size of the latent space\n",
        "    self.shape = shape                   # Store the original input shape\n",
        "\n",
        "    # Encoder: flattens input and encodes to latent_dim\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),                  # Flatten input to 1D\n",
        "      layers.Dense(latent_dim, activation='relu'),  # Dense layer for encoding\n",
        "    ])\n",
        "\n",
        "    # Decoder: reconstructs original shape from latent vector\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),  # Dense layer to expand back to original size\n",
        "      layers.Reshape(shape)               # Reshape output to original input shape\n",
        "    ])\n",
        "\n",
        "  # Forward pass: encode then decode\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)             # Encode input\n",
        "    decoded = self.decoder(encoded)       # Decode latent vector\n",
        "    return decoded                        # Return reconstruction\n",
        "\n",
        "# Set the shape and latent dimension for the autoencoder\n",
        "shape = x_test.shape[1:]                  # Get shape of input images (e.g., (28, 28))\n",
        "latent_dim = 64                           # Set size of latent space\n",
        "\n",
        "# Instantiate the Autoencoder model\n",
        "autoencoder = Autoencoder(latent_dim, shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psqXTvheGJUp"
      },
      "source": [
        "Mean Squared Error (MSE) is commonly used as the loss function for autoencoders working with images because it measures the average squared difference between the original and reconstructed pixel values. This is appropriate for images because:\n",
        "\n",
        "- **Pixel-wise similarity:** MSE penalizes large differences between corresponding pixels, encouraging the autoencoder to produce reconstructions that are visually similar to the input.\n",
        "- **Continuous values:** Image data is often represented as continuous values (e.g., pixel intensities between 0 and 1), making MSE a natural choice.\n",
        "- **Smooth gradients:** MSE provides smooth and stable gradients, which helps neural networks learn effectively during training.\n",
        "\n",
        "In summary, MSE is simple, effective, and aligns well with the goal of minimizing reconstruction error in image autoencoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:29.200930Z",
          "iopub.status.busy": "2024-08-16T05:08:29.200574Z",
          "iopub.status.idle": "2024-08-16T05:08:29.216831Z",
          "shell.execute_reply": "2024-08-16T05:08:29.216260Z"
        },
        "id": "9I1JlqEIDCI4"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oJSeMTroABs"
      },
      "source": [
        "Train the model using `x_train` as both the input and the target. The `encoder` will learn to compress the dataset from 784 dimensions to the latent space, and the `decoder` will learn to reconstruct the original images.\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:29.220373Z",
          "iopub.status.busy": "2024-08-16T05:08:29.219777Z",
          "iopub.status.idle": "2024-08-16T05:08:56.658179Z",
          "shell.execute_reply": "2024-08-16T05:08:56.657521Z"
        },
        "id": "h1RI9OfHDBsK"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAM1QBhtoC-n"
      },
      "source": [
        "Now that the model is trained, let's test it by encoding and decoding images from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:56.661491Z",
          "iopub.status.busy": "2024-08-16T05:08:56.661241Z",
          "iopub.status.idle": "2024-08-16T05:08:56.830984Z",
          "shell.execute_reply": "2024-08-16T05:08:56.830252Z"
        },
        "id": "Pbr5WCj7FQUi"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:56.835301Z",
          "iopub.status.busy": "2024-08-16T05:08:56.834660Z",
          "iopub.status.idle": "2024-08-16T05:08:57.272919Z",
          "shell.execute_reply": "2024-08-16T05:08:57.272284Z"
        },
        "id": "s4LlDOS6FUA1"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstructed\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4gv6G8PoRQE"
      },
      "source": [
        "### Second example: Image denoising\n",
        "\n",
        "\n",
        "![Image denoising results](https://www.tensorflow.org/static/tutorials/generative/images/image_denoise_fmnist_results.png)\n",
        "\n",
        "An autoencoder can also be trained to remove noise from images. In the following section, you will create a noisy version of the Fashion MNIST dataset by applying random noise to each image. You will then train an autoencoder using the noisy image as input, and the original image as the target.\n",
        "\n",
        "Let's reimport the dataset to omit the modifications made earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:57.276905Z",
          "iopub.status.busy": "2024-08-16T05:08:57.276501Z",
          "iopub.status.idle": "2024-08-16T05:08:57.631164Z",
          "shell.execute_reply": "2024-08-16T05:08:57.630455Z"
        },
        "id": "gDYHJA2PCQ3m"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:57.635346Z",
          "iopub.status.busy": "2024-08-16T05:08:57.634768Z",
          "iopub.status.idle": "2024-08-16T05:08:57.724216Z",
          "shell.execute_reply": "2024-08-16T05:08:57.723450Z"
        },
        "id": "uJZ-TcaqDBr5"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPZl_6P65_8R"
      },
      "source": [
        "Adding random noise to the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:57.727602Z",
          "iopub.status.busy": "2024-08-16T05:08:57.727346Z",
          "iopub.status.idle": "2024-08-16T05:08:58.694844Z",
          "shell.execute_reply": "2024-08-16T05:08:58.693947Z"
        },
        "id": "axSMyxC354fc"
      },
      "outputs": [],
      "source": [
        "noise_factor = 0.2  # Set the amount of noise to add\n",
        "\n",
        "# Add random Gaussian noise to the training images\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\n",
        "# Add random Gaussian noise to the test images\n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n",
        "\n",
        "# Clip the noisy training images to be between 0 and 1\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "# Clip the noisy test images to be between 0 and 1\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRxHe4XXltNd"
      },
      "source": [
        "Plot the noisy images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:58.699206Z",
          "iopub.status.busy": "2024-08-16T05:08:58.698575Z",
          "iopub.status.idle": "2024-08-16T05:08:59.347039Z",
          "shell.execute_reply": "2024-08-16T05:08:59.346397Z"
        },
        "id": "thKUmbVVCQpt"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.title(\"original + noise\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy9SY8jGl5aP"
      },
      "source": [
        "#### Define a convolutional autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT_BhZngWMwp"
      },
      "source": [
        "In this example, you will train a convolutional autoencoder using  [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers in the `encoder`, and [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) layers in the `decoder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox3mqTqAGJUr"
      },
      "source": [
        "`Conv2DTranspose` is a type of convolutional layer often called a \"deconvolution\" or \"upsampling\" layer. It performs the reverse operation of a standard `Conv2D` layer: instead of reducing the spatial dimensions (height and width) of the input, it increases them.\n",
        "\n",
        "- **Conv2D**: Used in the encoder part of an autoencoder to extract features and reduce the spatial size of the input (downsampling).\n",
        "- **Conv2DTranspose**: Used in the decoder part to reconstruct the original image size from the compressed representation (upsampling).\n",
        "\n",
        "**Why not use Conv2D in the decoder?**\n",
        "\n",
        "- `Conv2D` reduces spatial dimensions, which is the opposite of what we want in the decoder.\n",
        "- The decoder needs to upsample (increase) the spatial dimensions to reconstruct the original image, which is exactly what `Conv2DTranspose` does.\n",
        "\n",
        "In summary, use `Conv2D` for downsampling (encoder) and `Conv2DTranspose` for upsampling (decoder) in convolutional autoencoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:59.350671Z",
          "iopub.status.busy": "2024-08-16T05:08:59.350427Z",
          "iopub.status.idle": "2024-08-16T05:08:59.378767Z",
          "shell.execute_reply": "2024-08-16T05:08:59.378096Z"
        },
        "id": "R5KjoIlYCQko"
      },
      "outputs": [],
      "source": [
        "# Define a convolutional autoencoder model for image denoising\n",
        "class Denoise(Model):\n",
        "  def __init__(self):\n",
        "    super(Denoise, self).__init__()  # Initialize the base Model class\n",
        "\n",
        "    # Define the encoder as a Sequential model\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Input(shape=(28, 28, 1)),  # Input layer for 28x28 grayscale images\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),  # Downsample with 16 filters\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)    # Further downsample with 8 filters\n",
        "    ])\n",
        "\n",
        "    # Define the decoder as a Sequential model\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),   # Upsample with 8 filters\n",
        "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),  # Further upsample with 16 filters\n",
        "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')                # Output layer to reconstruct the image\n",
        "    ])\n",
        "\n",
        "  # Define the forward pass\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)   # Pass input through encoder\n",
        "    decoded = self.decoder(encoded)  # Pass encoded output through decoder\n",
        "    return decoded             # Return the reconstructed image\n",
        "\n",
        "# Instantiate the Denoise autoencoder model\n",
        "autoencoder = Denoise()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:59.381995Z",
          "iopub.status.busy": "2024-08-16T05:08:59.381461Z",
          "iopub.status.idle": "2024-08-16T05:08:59.387699Z",
          "shell.execute_reply": "2024-08-16T05:08:59.387125Z"
        },
        "id": "QYKbiDFYCQfj"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:08:59.390711Z",
          "iopub.status.busy": "2024-08-16T05:08:59.390349Z",
          "iopub.status.idle": "2024-08-16T05:09:40.702609Z",
          "shell.execute_reply": "2024-08-16T05:09:40.701794Z"
        },
        "id": "IssFr1BNCQX3"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85xUVBGTAKp"
      },
      "source": [
        "Let's take a look at a summary of the encoder. Notice how the images are downsampled from 28x28 to 7x7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:40.706339Z",
          "iopub.status.busy": "2024-08-16T05:09:40.705669Z",
          "iopub.status.idle": "2024-08-16T05:09:40.718983Z",
          "shell.execute_reply": "2024-08-16T05:09:40.718268Z"
        },
        "id": "oEpxlX6sTEQz"
      },
      "outputs": [],
      "source": [
        "autoencoder.encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDZBfMx1UtXx"
      },
      "source": [
        "The decoder upsamples the images back from 7x7 to 28x28."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:40.722189Z",
          "iopub.status.busy": "2024-08-16T05:09:40.721682Z",
          "iopub.status.idle": "2024-08-16T05:09:40.733954Z",
          "shell.execute_reply": "2024-08-16T05:09:40.733322Z"
        },
        "id": "pbeQtYMaUpro"
      },
      "outputs": [],
      "source": [
        "autoencoder.decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7-VAuEy_N6M"
      },
      "source": [
        "Plotting both the noisy images and the denoised images produced by the autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:40.737318Z",
          "iopub.status.busy": "2024-08-16T05:09:40.736788Z",
          "iopub.status.idle": "2024-08-16T05:09:41.827816Z",
          "shell.execute_reply": "2024-08-16T05:09:41.827103Z"
        },
        "id": "t5IyPi1fCQQz"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:41.830952Z",
          "iopub.status.busy": "2024-08-16T05:09:41.830700Z",
          "iopub.status.idle": "2024-08-16T05:09:42.404449Z",
          "shell.execute_reply": "2024-08-16T05:09:42.403784Z"
        },
        "id": "sfxr9NdBCP_x"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "\n",
        "    # display original + noise\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.title(\"original + noise\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    bx = plt.subplot(2, n, i + n + 1)\n",
        "    plt.title(\"reconstructed\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErGrTnWHoUYl"
      },
      "source": [
        "### Third example: Anomaly detection\n",
        "\n",
        "#### Overview\n",
        "\n",
        "\n",
        "In this example, you will train an autoencoder to detect anomalies on the [ECG5000 dataset](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000). This dataset contains 5,000 [Electrocardiograms](https://en.wikipedia.org/wiki/Electrocardiography), each with 140 data points. You will use a simplified version of the dataset, where each example has been labeled either `0` (corresponding to an abnormal rhythm), or `1` (corresponding to a normal rhythm). You are interested in identifying the abnormal rhythms.\n",
        "\n",
        "Note: This is a labeled dataset, so you could phrase this as a supervised learning problem. The goal of this example is to illustrate anomaly detection concepts you can apply to larger datasets, where you do not have labels available (for example, if you had many thousands of normal rhythms, and only a small number of abnormal rhythms).\n",
        "\n",
        "How will you detect anomalies using an autoencoder? Recall that an autoencoder is trained to minimize reconstruction error. You will train an autoencoder on the normal rhythms only, then use it to reconstruct all the data. Our hypothesis is that the abnormal rhythms will have higher reconstruction error. You will then classify a rhythm as an anomaly if the reconstruction error surpasses a fixed threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5estNaur_Mh"
      },
      "source": [
        "#### Load ECG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y35nsXLPsDNX"
      },
      "source": [
        "The dataset you will use is based on one from [timeseriesclassification.com](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI2x6rudGJUz"
      },
      "source": [
        "**What is ECG data?**\n",
        "\n",
        "ECG (Electrocardiogram) data is a time series recording of the electrical activity of the heart. Each ECG sample typically consists of a sequence of voltage measurements taken at regular intervals, representing the heart's rhythm and electrical conduction patterns. These signals are used by clinicians to detect and diagnose various cardiac conditions, such as arrhythmias, heart attacks, and other abnormalities.\n",
        "\n",
        "In machine learning, ECG data is often represented as a 1D array (or vector) of numerical values, where each value corresponds to the electrical potential measured at a specific time point. For example, in the ECG5000 dataset used above, each ECG record contains 140 data points. The dataset may also include labels indicating whether the rhythm is normal or abnormal, which can be used for supervised or unsupervised learning tasks such as anomaly detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.408208Z",
          "iopub.status.busy": "2024-08-16T05:09:42.407921Z",
          "iopub.status.idle": "2024-08-16T05:09:42.795830Z",
          "shell.execute_reply": "2024-08-16T05:09:42.795151Z"
        },
        "id": "KmKRDJWgsFYa"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "raw_data = dataframe.values\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.799351Z",
          "iopub.status.busy": "2024-08-16T05:09:42.798876Z",
          "iopub.status.idle": "2024-08-16T05:09:42.806296Z",
          "shell.execute_reply": "2024-08-16T05:09:42.805696Z"
        },
        "id": "UmuCPVYKsKKx"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the electrocadriogram data\n",
        "data = raw_data[:, 0:-1]\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=21\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byK2vP7hsMbz"
      },
      "source": [
        "Normalize the data to `[0,1]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.809727Z",
          "iopub.status.busy": "2024-08-16T05:09:42.809284Z",
          "iopub.status.idle": "2024-08-16T05:09:42.834425Z",
          "shell.execute_reply": "2024-08-16T05:09:42.833790Z"
        },
        "id": "tgMZVWRKsPx6"
      },
      "outputs": [],
      "source": [
        "min_val = tf.reduce_min(train_data)\n",
        "max_val = tf.reduce_max(train_data)\n",
        "\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "test_data = (test_data - min_val) / (max_val - min_val)\n",
        "\n",
        "train_data = tf.cast(train_data, tf.float32)\n",
        "test_data = tf.cast(test_data, tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdSYr2IPsTiz"
      },
      "source": [
        "You will train the autoencoder using only the normal rhythms, which are labeled in this dataset as `1`. Separate the normal rhythms from the abnormal rhythms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.837735Z",
          "iopub.status.busy": "2024-08-16T05:09:42.837314Z",
          "iopub.status.idle": "2024-08-16T05:09:42.865454Z",
          "shell.execute_reply": "2024-08-16T05:09:42.864854Z"
        },
        "id": "VvK4NRe8sVhE"
      },
      "outputs": [],
      "source": [
        "train_labels = train_labels.astype(bool)\n",
        "test_labels = test_labels.astype(bool)\n",
        "\n",
        "normal_train_data = train_data[train_labels]\n",
        "normal_test_data = test_data[test_labels]\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_test_data = test_data[~test_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVcTBDo-CqFS"
      },
      "source": [
        "Plot a normal ECG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.869047Z",
          "iopub.status.busy": "2024-08-16T05:09:42.868563Z",
          "iopub.status.idle": "2024-08-16T05:09:42.990789Z",
          "shell.execute_reply": "2024-08-16T05:09:42.990187Z"
        },
        "id": "ZTlMIrpmseYe"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), normal_train_data[0])\n",
        "plt.title(\"A Normal ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpI9by2ZA0NN"
      },
      "source": [
        "Plot an anomalous ECG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:42.993975Z",
          "iopub.status.busy": "2024-08-16T05:09:42.993439Z",
          "iopub.status.idle": "2024-08-16T05:09:43.102433Z",
          "shell.execute_reply": "2024-08-16T05:09:43.101784Z"
        },
        "id": "zrpXREF2siBr"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), anomalous_train_data[0])\n",
        "plt.title(\"An Anomalous ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DS6QKZJslZz"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:43.105538Z",
          "iopub.status.busy": "2024-08-16T05:09:43.105139Z",
          "iopub.status.idle": "2024-08-16T05:09:43.115699Z",
          "shell.execute_reply": "2024-08-16T05:09:43.115113Z"
        },
        "id": "bf6owZQDsp9y"
      },
      "outputs": [],
      "source": [
        "class AnomalyDetector(Model):\n",
        "  def __init__(self):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Dense(32, activation=\"relu\"),\n",
        "      layers.Dense(16, activation=\"relu\"),\n",
        "      layers.Dense(8, activation=\"relu\")])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(16, activation=\"relu\"),\n",
        "      layers.Dense(32, activation=\"relu\"),\n",
        "      layers.Dense(140, activation=\"sigmoid\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = AnomalyDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:43.119002Z",
          "iopub.status.busy": "2024-08-16T05:09:43.118401Z",
          "iopub.status.idle": "2024-08-16T05:09:43.124837Z",
          "shell.execute_reply": "2024-08-16T05:09:43.124256Z"
        },
        "id": "gwRpBBbg463S"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mae')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuTy60STBEy4"
      },
      "source": [
        "Notice that the autoencoder is trained using only the normal ECGs, but is evaluated using the full test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:43.128078Z",
          "iopub.status.busy": "2024-08-16T05:09:43.127650Z",
          "iopub.status.idle": "2024-08-16T05:09:50.219910Z",
          "shell.execute_reply": "2024-08-16T05:09:50.219194Z"
        },
        "id": "V6NFSs-jsty2"
      },
      "outputs": [],
      "source": [
        "history = autoencoder.fit(normal_train_data, normal_train_data,\n",
        "          epochs=20,\n",
        "          batch_size=512,\n",
        "          validation_data=(test_data, test_data),\n",
        "          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:50.223542Z",
          "iopub.status.busy": "2024-08-16T05:09:50.222859Z",
          "iopub.status.idle": "2024-08-16T05:09:50.372600Z",
          "shell.execute_reply": "2024-08-16T05:09:50.372011Z"
        },
        "id": "OEexphFwwTQS"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceI5lKv1BT-A"
      },
      "source": [
        "You will soon classify an ECG as anomalous if the reconstruction error is greater than one standard deviation from the normal training examples. First, let's plot a normal ECG from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:50.376353Z",
          "iopub.status.busy": "2024-08-16T05:09:50.375857Z",
          "iopub.status.idle": "2024-08-16T05:09:50.512876Z",
          "shell.execute_reply": "2024-08-16T05:09:50.512258Z"
        },
        "id": "hmsk4DuktxJ2"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.encoder(normal_test_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(normal_test_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(140), decoded_data[0], normal_test_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocA_q9ufB_aF"
      },
      "source": [
        "Create a similar plot, this time for an anomalous test example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:50.516237Z",
          "iopub.status.busy": "2024-08-16T05:09:50.515971Z",
          "iopub.status.idle": "2024-08-16T05:09:50.644650Z",
          "shell.execute_reply": "2024-08-16T05:09:50.644059Z"
        },
        "id": "vNFTuPhLwTBn"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(anomalous_test_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(140), decoded_data[0], anomalous_test_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocimg3MBswdS"
      },
      "source": [
        "#### Detect anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnh8wmkDsypN"
      },
      "source": [
        "Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeuT8uTA5Y_w"
      },
      "source": [
        "Plot the reconstruction error on normal ECGs from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:50.648521Z",
          "iopub.status.busy": "2024-08-16T05:09:50.647842Z",
          "iopub.status.idle": "2024-08-16T05:09:51.700226Z",
          "shell.execute_reply": "2024-08-16T05:09:51.699544Z"
        },
        "id": "N7FltOnHu4-l"
      },
      "outputs": [],
      "source": [
        "reconstructions = autoencoder.predict(normal_train_data)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
        "\n",
        "plt.hist(train_loss[None,:], bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh-3ChEF5hog"
      },
      "source": [
        "Choose a threshold value that is one standard deviations above the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:51.703569Z",
          "iopub.status.busy": "2024-08-16T05:09:51.703303Z",
          "iopub.status.idle": "2024-08-16T05:09:51.707434Z",
          "shell.execute_reply": "2024-08-16T05:09:51.706820Z"
        },
        "id": "82hkl0Chs3P_"
      },
      "outputs": [],
      "source": [
        "threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "print(\"Threshold: \", threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEGlA1Be50Nj"
      },
      "source": [
        "Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLSDAeb51D_"
      },
      "source": [
        "If you examine the reconstruction error for the anomalous examples in the test set, you'll notice most have greater reconstruction error than the threshold. By varing the threshold, you can adjust the [precision](https://developers.google.com/machine-learning/glossary#precision) and [recall](https://developers.google.com/machine-learning/glossary#recall) of your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:51.711181Z",
          "iopub.status.busy": "2024-08-16T05:09:51.710568Z",
          "iopub.status.idle": "2024-08-16T05:09:52.188974Z",
          "shell.execute_reply": "2024-08-16T05:09:52.188258Z"
        },
        "id": "sKVwjQK955Wy"
      },
      "outputs": [],
      "source": [
        "reconstructions = autoencoder.predict(anomalous_test_data)\n",
        "test_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n",
        "\n",
        "plt.hist(test_loss[None, :], bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFVk_XGE6AX2"
      },
      "source": [
        "Classify an ECG as an anomaly if the reconstruction error is greater than the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:52.192466Z",
          "iopub.status.busy": "2024-08-16T05:09:52.192176Z",
          "iopub.status.idle": "2024-08-16T05:09:52.196762Z",
          "shell.execute_reply": "2024-08-16T05:09:52.196153Z"
        },
        "id": "mkgJZfhh6CHr"
      },
      "outputs": [],
      "source": [
        "def predict(model, data, threshold):\n",
        "  # Use the model to reconstruct the input data\n",
        "  reconstructions = model(data)\n",
        "  # Calculate the mean absolute error between the reconstructions and the original data\n",
        "  loss = tf.keras.losses.mae(reconstructions, data)\n",
        "  # Return True if the loss is less than the threshold (i.e., normal), otherwise False (anomaly)\n",
        "  return tf.math.less(loss, threshold)\n",
        "\n",
        "def print_stats(predictions, labels):\n",
        "  # Print the accuracy of the predictions compared to the true labels\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
        "  # Print the precision of the predictions\n",
        "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
        "  # Print the recall of the predictions\n",
        "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-16T05:09:52.200104Z",
          "iopub.status.busy": "2024-08-16T05:09:52.199499Z",
          "iopub.status.idle": "2024-08-16T05:09:52.217458Z",
          "shell.execute_reply": "2024-08-16T05:09:52.216884Z"
        },
        "id": "sOcfXfXq6FBd"
      },
      "outputs": [],
      "source": [
        "preds = predict(autoencoder, test_data, threshold)\n",
        "print_stats(preds, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH48mTfcGJU-"
      },
      "source": [
        "## pix2pix: Image-to-image translation with a conditional GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaVfBc8tGJU-"
      },
      "source": [
        "This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in [Image-to-image translation with conditional adversarial networks](https://arxiv.org/abs/1611.07004) by Isola et al. (2017). pix2pix is not application specific‚Äîit can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white images, turning Google Maps photos into aerial images, and even transforming sketches into photos.\n",
        "\n",
        "In this example, your network will generate images of building facades using the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/) provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep it short, you will use a [preprocessed copy](https://efrosgans.eecs.berkeley.edu/pix2pix/datasets/) of this dataset created by the pix2pix authors.\n",
        "\n",
        "In the pix2pix cGAN, you condition on input images and generate corresponding output images. cGANs were first proposed in [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (Mirza and Osindero, 2014)\n",
        "\n",
        "The architecture of your network will contain:\n",
        "\n",
        "- A generator with a [U-Net](https://arxiv.org/abs/1505.04597)-based architecture.\n",
        "- A discriminator represented by a convolutional PatchGAN classifier (proposed in the [pix2pix paper](https://arxiv.org/abs/1611.07004)).\n",
        "\n",
        "Note that each epoch can take around 15 seconds on a single V100 GPU.\n",
        "\n",
        "Below are some examples of the output generated by the pix2pix cGAN after training for 200 epochs on the facades dataset (80k steps).\n",
        "\n",
        "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
        "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkAYk1KdGJU-"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNub64beGJU_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH_dxmWFGJU_"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Download the CMP Facade Database data (30MB). Additional datasets are available in the same format [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/). In Colab you can select other datasets from the drop-down menu. Note that some of the other datasets are significantly larger (`edges2handbags` is 8GB in size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcDjl67NGJU_"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"facades\" #@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cTAffX8GJU_"
      },
      "outputs": [],
      "source": [
        "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    fname=f\"{dataset_name}.tar.gz\",\n",
        "    origin=_URL,\n",
        "    extract=True)\n",
        "\n",
        "path_to_zip  = pathlib.Path(path_to_zip)\n",
        "\n",
        "PATH = path_to_zip/dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnUoXrCaGJU_"
      },
      "outputs": [],
      "source": [
        "list(PATH.parent.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngQzU7wJGJU_"
      },
      "source": [
        "Each original image is of size `256 x 512` containing two `256 x 256` images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnra0KzlGJVA"
      },
      "outputs": [],
      "source": [
        "sample_img_path = str(PATH / os.path.join('train', '1.jpg'))\n",
        "print(sample_img_path)\n",
        "sample_image = tf.io.read_file(sample_img_path)\n",
        "sample_image = tf.io.decode_jpeg(sample_image)\n",
        "print(sample_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K721i0FGJVA"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2bIZCBrGJVA"
      },
      "source": [
        "You need to separate real building facade images from the architecture label images‚Äîall of which will be of size `256 x 256`.\n",
        "\n",
        "Define a function that loads image files and outputs two image tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCjR5wDGJVA"
      },
      "outputs": [],
      "source": [
        "def load(image_file):\n",
        "  # Read and decode an image file to a uint8 tensor\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "\n",
        "  # Split each image tensor into two tensors:\n",
        "  # - one with a real building facade image\n",
        "  # - one with an architecture label image\n",
        "  w = tf.shape(image)[1]\n",
        "  w = w // 2\n",
        "  input_image = image[:, w:, :]\n",
        "  real_image = image[:, :w, :]\n",
        "\n",
        "  # Convert both images to float32 tensors\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YRYK0jGJVB"
      },
      "source": [
        "Plot a sample of the input (architecture label image) and real (building facade photo) images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbcWD3EPGJVB"
      },
      "outputs": [],
      "source": [
        "inp, re = load(str(PATH / 'train/100.jpg'))\n",
        "# Casting to int for matplotlib to display the images\n",
        "plt.figure()\n",
        "plt.imshow(inp / 255.0)\n",
        "plt.figure()\n",
        "plt.imshow(re / 255.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt4klr8eGJVB"
      },
      "source": [
        "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n",
        "\n",
        "Define several functions that:\n",
        "\n",
        "1. Resize each `256 x 256` image to a larger height and width‚Äî`286 x 286`.\n",
        "2. Randomly crop it back to `256 x 256`.\n",
        "3. Randomly flip the image horizontally i.e., left to right (random mirroring).\n",
        "4. Normalize the images to the `[-1, 1]` range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6s1bOaEGJVB"
      },
      "source": [
        "**Jittering in pix2pix**\n",
        "\n",
        "Jittering is a data augmentation technique used in pix2pix to improve the robustness and generalization of the model. It involves randomly resizing the input images to a slightly larger size (e.g., from 256√ó256 to 286√ó286), then randomly cropping them back to the original size (256√ó256), and randomly flipping them horizontally.\n",
        "\n",
        "This process helps the model learn to handle small spatial variations and prevents overfitting by exposing it to more diverse training examples. Jittering is especially important in image-to-image translation tasks, where the model needs to generalize well to unseen data and not just memorize the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuukXLs8GJVC"
      },
      "outputs": [],
      "source": [
        "# The facade training set consist of 400 images\n",
        "BUFFER_SIZE = 400\n",
        "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
        "BATCH_SIZE = 1\n",
        "# Each image is 256x256 in size\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLMimxseGJVC"
      },
      "outputs": [],
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EymXF2YRGJVD"
      },
      "outputs": [],
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVP8AN9uGJVD"
      },
      "outputs": [],
      "source": [
        "# Normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n155s6EnGJVE"
      },
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # Resizing to 286x286\n",
        "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "\n",
        "  # Random cropping back to 256x256\n",
        "  input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    # Random mirroring\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00OueXPGJVE"
      },
      "source": [
        "You can inspect some of the preprocessed output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg8CxrsaGJVF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(4):\n",
        "  rj_inp, rj_re = random_jitter(inp, re)\n",
        "  plt.subplot(2, 2, i + 1)\n",
        "  plt.imshow(rj_inp / 255.0)\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gHWIvJ7GJVF"
      },
      "source": [
        "Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnEFiUJQGJVG"
      },
      "outputs": [],
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx-YuCkiGJVG"
      },
      "outputs": [],
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfGLfbwTGJVG"
      },
      "source": [
        "### Build an input pipeline with `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGXbP-DDGJVH"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\n",
        "train_dataset = train_dataset.map(load_image_train,\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZprZruVnGJVH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdgfPDiGJVH"
      },
      "source": [
        "### Build the generator\n",
        "\n",
        "The generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](../images/segmentation.ipynb) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)\n",
        "\n",
        "- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n",
        "- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n",
        "- There are skip connections between the encoder and decoder (as in the U-Net)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJcBumMeGJVI"
      },
      "source": [
        "Define the downsampler (encoder):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv177n6OGJVI"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXSln8tuGJVJ"
      },
      "outputs": [],
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  # Initialize the weights with a normal distribution\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  # Create a Sequential model\n",
        "  result = tf.keras.Sequential()\n",
        "  # Add a Conv2D layer with the given number of filters and kernel size\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  # Optionally add BatchNormalization for faster and more stable training\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  # Add LeakyReLU activation for non-linearity\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spx3kdd0GJVJ"
      },
      "outputs": [],
      "source": [
        "down_model = downsample(3, 4)\n",
        "down_result = down_model(tf.expand_dims(inp, 0))\n",
        "print (down_result.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9LBm0yoGJVJ"
      },
      "source": [
        "Define the upsampler (decoder):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPEvEuyzGJVJ"
      },
      "outputs": [],
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  # Initialize the weights with a normal distribution\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  # Create a Sequential model for the upsampling block\n",
        "  result = tf.keras.Sequential()\n",
        "  # Add a Conv2DTranspose layer to upsample the input\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  # Add BatchNormalization for faster and more stable training\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  # Optionally add Dropout for regularization (only in first 3 blocks of decoder)\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  # Add ReLU activation for non-linearity\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcBFjr0nGJVK"
      },
      "outputs": [],
      "source": [
        "up_model = upsample(3, 4)\n",
        "up_result = up_model(down_result)\n",
        "print (up_result.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa_EXqyWGJVK"
      },
      "source": [
        "Define the generator with the downsampler and the upsampler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1HWx68qGJVK"
      },
      "outputs": [],
      "source": [
        "def Generator():\n",
        "  # Define the input layer with shape (256, 256, 3)\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "  # Create the encoder (downsampling stack) using downsample blocks\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # First block, no batchnorm\n",
        "    downsample(128, 4),  # Second block\n",
        "    downsample(256, 4),  # Third block\n",
        "    downsample(512, 4),  # Fourth block\n",
        "    downsample(512, 4),  # Fifth block\n",
        "    downsample(512, 4),  # Sixth block\n",
        "    downsample(512, 4),  # Seventh block\n",
        "    downsample(512, 4),  # Eighth block\n",
        "  ]\n",
        "\n",
        "  # Create the decoder (upsampling stack) using upsample blocks\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True),  # First block, with dropout\n",
        "    upsample(512, 4, apply_dropout=True),  # Second block, with dropout\n",
        "    upsample(512, 4, apply_dropout=True),  # Third block, with dropout\n",
        "    upsample(512, 4),  # Fourth block\n",
        "    upsample(256, 4),  # Fifth block\n",
        "    upsample(128, 4),  # Sixth block\n",
        "    upsample(64, 4),   # Seventh block\n",
        "  ]\n",
        "\n",
        "  # Initialize the weights for the last layer\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  # Define the last layer to get the output image with tanh activation\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      OUTPUT_CHANNELS, 4,\n",
        "      strides=2,\n",
        "      padding='same',\n",
        "      kernel_initializer=initializer,\n",
        "      activation='tanh')  # Output shape: (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs  # Start with the input\n",
        "\n",
        "  # Downsampling through the encoder, saving skip connections\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)      # Apply downsampling block\n",
        "    skips.append(x)  # Save output for skip connection\n",
        "\n",
        "  # Reverse all but the last skip for use in upsampling\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and adding skip connections from encoder\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)                              # Apply upsampling block\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])  # Add skip connection\n",
        "\n",
        "  x = last(x)  # Apply the last layer to get the final output\n",
        "\n",
        "  # Return the Keras Model\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdPa2a55GJVL"
      },
      "source": [
        "Visualize the generator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRexhUC8GJVL"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQzpP2f6GJVL"
      },
      "source": [
        "Test the generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd7wjcj9GJVM"
      },
      "outputs": [],
      "source": [
        "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
        "plt.imshow(gen_output[0, ...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J92NeLq3GJVM"
      },
      "source": [
        "#### Define the generator loss\n",
        "\n",
        "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
        "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
        "- This allows the generated image to become structurally similar to the target image.\n",
        "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XboOc6rGGJVM"
      },
      "outputs": [],
      "source": [
        "LAMBDA = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLfz6fZFGJVN"
      },
      "source": [
        "#### Why BinaryCrossentropy Loss is Needed in pix2pix\n",
        "\n",
        "In pix2pix, a conditional GAN (cGAN) is used for image-to-image translation. The GAN framework consists of two neural networks: the **generator** and the **discriminator**. The generator tries to produce realistic images from input data, while the discriminator tries to distinguish between real images and those produced by the generator.\n",
        "\n",
        "##### BinaryCrossentropy Loss\n",
        "\n",
        "The **BinaryCrossentropy** loss is used because the discriminator's task is a binary classification: it must decide whether each image patch is real (from the dataset) or fake (generated). In pix2pix, the discriminator is a PatchGAN, which outputs a matrix where each element corresponds to a small patch of the input image. Each patch is classified as real or fake, so BinaryCrossentropy is applied to every patch.\n",
        "\n",
        "##### Does the Discriminator Classify the Whole Image or Each Pixel?\n",
        "\n",
        "The discriminator does **not** classify the whole image as real or fake, nor does it classify each pixel individually. Instead, it classifies overlapping patches (e.g., 70x70 pixels) within the image. The output is a grid (e.g., 30x30) of predictions, each representing the probability that a corresponding patch is real. This encourages the generator to produce realistic details at the patch level.\n",
        "\n",
        "##### Generator Loss\n",
        "\n",
        "The generator's loss has two components:\n",
        "- **GAN loss**: Encourages the generator to produce images that the discriminator classifies as real (for all patches). This is computed as BinaryCrossentropy between an array of ones (targeting \"real\") and the discriminator's output for generated images.\n",
        "- **L1 loss**: Measures the mean absolute error between the generated image and the target image, encouraging structural similarity.\n",
        "\n",
        "The total generator loss is:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxyn8ZHjGJVN"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtAESJ-pGJVN"
      },
      "outputs": [],
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  # GAN loss: how well the generator fools the discriminator\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # L1 loss: mean absolute error between generated image and target image\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  # Total generator loss: GAN loss + weighted L1 loss\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  # Return all losses for logging and optimization\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjwR9cnNGJVN"
      },
      "source": [
        "The training procedure for the generator is as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4IVKlUPGJVN"
      },
      "source": [
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLB7CmCSGJVO"
      },
      "source": [
        "### Build the discriminator\n",
        "\n",
        "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier‚Äîit tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
        "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
        "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
        "- The discriminator receives 2 inputs:\n",
        "    - The input image and the target image, which it should classify as real.\n",
        "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
        "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eySbte0ZGJVO"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  # Input layers for the input image and the target image\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  # Concatenate the input and target images along the channel axis\n",
        "  # Shape: (batch_size, 256, 256, 6)\n",
        "  x = tf.keras.layers.concatenate([inp, tar])\n",
        "\n",
        "  # First downsampling block: Conv2D -> (optional) BatchNorm -> LeakyReLU\n",
        "  # Reduces spatial size, increases channels\n",
        "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "\n",
        "  # Second downsampling block\n",
        "  down2 = downsample(128, 4)(down1)    # (batch_size, 64, 64, 128)\n",
        "\n",
        "  # Third downsampling block\n",
        "  down3 = downsample(256, 4)(down2)    # (batch_size, 32, 32, 256)\n",
        "\n",
        "  # Zero padding to increase spatial dimensions\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "\n",
        "  # Convolution to extract features, stride=1 keeps spatial size\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "\n",
        "  # Batch normalization for stable training\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  # LeakyReLU activation for non-linearity\n",
        "  # LeakyReLU is used instead of ReLU to avoid the \"dying ReLU\" problem,\n",
        "  # where neurons can become inactive and only output zero. LeakyReLU allows\n",
        "  # a small, non-zero gradient when the unit is not active, which helps gradients\n",
        "  # flow through the network and improves training stability for GANs.\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  # Zero padding before the last layer\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "  # Final convolution: outputs a single-channel prediction map\n",
        "  # Each value represents real/fake for a patch\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "  # Return the Keras Model\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1D0D0YgGJVO"
      },
      "source": [
        "Visualize the discriminator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqxt5wrUGJVO"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p43kPT2GJVO"
      },
      "source": [
        "Test the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTCVS-v8GJVP"
      },
      "outputs": [],
      "source": [
        "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
        "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBfksIxLGJVP"
      },
      "source": [
        "#### Define the discriminator loss\n",
        "\n",
        "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
        "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
        "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
        "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ4Q0JhdGJVP"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  # Calculate loss for real images (should be classified as real/ones)\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  # Calculate loss for generated images (should be classified as fake/zeros)\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # Total discriminator loss is the sum of real and generated losses\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1H-ZfpGJVP"
      },
      "source": [
        "The training procedure for the discriminator is shown below.\n",
        "\n",
        "To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwkgtsN3GJVP"
      },
      "source": [
        "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CyKeslWGJVP"
      },
      "source": [
        "### Define the optimizers and a checkpoint-saver\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ci7JB7NGJVQ"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09wzJZ2nGJVQ"
      },
      "outputs": [],
      "source": [
        "# Directory where checkpoints will be saved during training\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Prefix for checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "# Create a TensorFlow checkpoint object to manage saving and restoring models and optimizers\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,      # Save generator optimizer state\n",
        "    discriminator_optimizer=discriminator_optimizer,  # Save discriminator optimizer state\n",
        "    generator=generator,                         # Save generator model weights\n",
        "    discriminator=discriminator                  # Save discriminator model weights\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kiLuEo8GJVQ"
      },
      "source": [
        "### Generate images\n",
        "\n",
        "Write a function to plot some images during training.\n",
        "\n",
        "- Pass images from the test set to the generator.\n",
        "- The generator will then translate the input image into the output.\n",
        "- The last step is to plot the predictions and _voila_!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_fZIQNGJVQ"
      },
      "source": [
        "Note: The `training=True` is intentional here since you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze5M9dPOGJVQ"
      },
      "outputs": [],
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPzrzYY1GJVQ"
      },
      "source": [
        "Test the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLATVTFpGJVR"
      },
      "outputs": [],
      "source": [
        "for example_input, example_target in test_dataset.take(1):\n",
        "  generate_images(generator, example_input, example_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFQ2nstTGJVR"
      },
      "source": [
        "### Training\n",
        "\n",
        "- For each example input generates an output.\n",
        "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
        "- Next, calculate the generator and the discriminator loss.\n",
        "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "- Finally, log the losses to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9z7JmQ9GJVR"
      },
      "outputs": [],
      "source": [
        "# Set the directory for TensorBoard logs\n",
        "log_dir = \"logs/\"\n",
        "\n",
        "# Create a summary writer for TensorBoard.\n",
        "# The logs will be saved in a subdirectory named with the current date and time.\n",
        "# This allows you to visualize training metrics in TensorBoard.\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds70gKzxGJVR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, step):\n",
        "  # Record operations for automatic differentiation for generator and discriminator\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    # Generate an output image from the input using the generator (forward pass)\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    # Get discriminator's output for real image pairs (input and ground truth)\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    # Get discriminator's output for fake image pairs (input and generated output)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    # Compute generator losses: total loss, GAN loss, and L1 loss\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
        "        disc_generated_output, gen_output, target)\n",
        "    # Compute discriminator loss (real vs. fake)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  # Calculate gradients of generator loss w.r.t. generator's trainable variables\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  # Calculate gradients of discriminator loss w.r.t. discriminator's trainable variables\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  # Apply gradients to update generator weights\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  # Apply gradients to update discriminator weights\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  # Write loss values to TensorBoard for visualization\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fI-4iSJGJVR"
      },
      "source": [
        "The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n",
        "\n",
        "- Iterates over the number of steps.\n",
        "- Every 10 steps print a dot (`.`).\n",
        "- Every 1k steps: clear the display and run `generate_images` to show the progress.\n",
        "- Every 5k steps: save a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_81aSv_4GJVR"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "  # Get one example input and target from the test dataset for visualization\n",
        "  example_input, example_target = next(iter(test_ds.take(1)))\n",
        "  # Record the start time for timing training steps\n",
        "  start = time.time()\n",
        "\n",
        "  # Iterate over the training dataset for the specified number of steps\n",
        "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "    # Every 1000 steps, clear the output and display progress\n",
        "    if (step) % 1000 == 0:\n",
        "      # Clear previous output in the notebook for a cleaner display\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      # If not the first step, print the time taken for the last 1000 steps\n",
        "      if step != 0:\n",
        "        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "      # Reset the timer for the next 1000 steps\n",
        "      start = time.time()\n",
        "\n",
        "      # Generate and display images using the generator for visual progress\n",
        "      generate_images(generator, example_input, example_target)\n",
        "      # Print the current step in thousands (k)\n",
        "      print(f\"Step: {step//1000}k\")\n",
        "\n",
        "    # Perform one training step (update generator and discriminator)\n",
        "    train_step(input_image, target, step)\n",
        "\n",
        "    # Print a dot every 10 steps to indicate progress\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.', end='', flush=True)\n",
        "\n",
        "    # Save a checkpoint every 5000 steps to preserve model state\n",
        "    if (step + 1) % 5000 == 0:\n",
        "      checkpoint.save(file_prefix=checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX1J454pGJVS"
      },
      "source": [
        "This training loop saves logs that you can view in TensorBoard to monitor the training progress.\n",
        "\n",
        "If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.\n",
        "\n",
        "Launch the TensorBoard viewer (Sorry, this doesn't\n",
        "display on tensorflow.org):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzb28vMYGJVS"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pUMMpjoGJVS"
      },
      "outputs": [],
      "source": [
        "fit(train_dataset, test_dataset, steps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjP9WCiGJVS"
      },
      "source": [
        "Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n",
        "\n",
        "- Check that neither the generator nor the discriminator model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
        "- The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n",
        "- For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.\n",
        "- For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.\n",
        "- As training progresses, the `gen_l1_loss` should go down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9VIqOvMGJVS"
      },
      "source": [
        "## Restore the latest checkpoint and test the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTyyxRg8GJVS"
      },
      "outputs": [],
      "source": [
        "!ls {checkpoint_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Uah_4KXGJVT"
      },
      "outputs": [],
      "source": [
        "# Restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB2-3GNVGJVT"
      },
      "source": [
        "## Generate some images using the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMyLxxX0GJVT"
      },
      "outputs": [],
      "source": [
        "# Run the trained model on a few examples from the test set\n",
        "for inp, tar in test_dataset.take(5):\n",
        "  generate_images(generator, inp, tar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dS5zGV7HHJ-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}